% introduction
@article{korenkevych2019autoregressive,
  title={Autoregressive policies for continuous control deep reinforcement learning},
  author={Korenkevych, Dmytro and Mahmood, A Rupam and Vasan, Gautham and Bergstra, James},
  journal={arXiv preprint arXiv:1903.11524},
  year={2019}
}
@article{haarnoja2018latent,
  title={Latent space policies for hierarchical reinforcement learning},
  author={Haarnoja, Tuomas and Hartikainen, Kristian and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1804.02808},
  year={2018}
}


% mdps.tex
@article{kass1998markov,
  title={Markov chain Monte Carlo in practice: a roundtable discussion},
  author={Kass, Robert E and Carlin, Bradley P and Gelman, Andrew and Neal, Radford M},
  journal={The American Statistician},
  volume={52},
  number={2},
  pages={93--100},
  year={1998},
  publisher={Taylor \& Francis Group}
}

@book{feinberg2012handbook,
  title={Handbook of Markov decision processes: methods and applications},
  author={Feinberg, Eugene A and Shwartz, Adam},
  volume={40},
  year={2012},
  publisher={Springer Science \& Business Media}
}
@article{bellman1956dynamic,
  title={Dynamic programming and Lagrange multipliers},
  author={Bellman, Richard},
  journal={Proceedings of the National Academy of Sciences of the United States of America},
  volume={42},
  number={10},
  pages={767},
  year={1956},
  publisher={National Academy of Sciences}
}
@article{arrow1949bayes,
  title={Bayes and minimax solutions of sequential decision problems},
  author={Arrow, Kenneth J and Blackwell, David and Girshick, Meyer A},
  journal={Econometrica, Journal of the Econometric Society},
  pages={213--244},
  year={1949},
  publisher={JSTOR}
}
@article{rosling1989optimal,
  title={Optimal inventory policies for assembly systems under random demands},
  author={Rosling, Kaj},
  journal={Operations Research},
  volume={37},
  number={4},
  pages={565--579},
  year={1989},
  publisher={INFORMS}
}
@article{shapley1953stochastic,
  title={Stochastic games},
  author={Shapley, Lloyd S},
  journal={Proceedings of the national academy of sciences},
  volume={39},
  number={10},
  pages={1095--1100},
  year={1953},
  publisher={National Acad Sciences}
}

@book{puterman2014markov,
  title={Markov decision processes: discrete stochastic dynamic programming},
  author={Puterman, Martin L},
  year={2014},
  publisher={John Wiley \& Sons}
}
@book{fleming2006controlled,
  title={Controlled Markov processes and viscosity solutions},
  author={Fleming, Wendell H and Soner, Halil Mete},
  volume={25},
  year={2006},
  publisher={Springer Science \& Business Media}
}
@article{howard1960dynamic,
  title={Dynamic programming and markov processes.},
  author={Howard, Ronald A},
  year={1960},
  publisher={John Wiley}
}
@article{piunovskiy2000constrained,
  title={Constrained Markovian decision processes: the dynamic programming approach},
  author={Piunovskiy, Alexei B and Mao, Xuerong},
  journal={Operations research letters},
  volume={27},
  number={3},
  pages={119--126},
  year={2000},
  publisher={Elsevier}
}
@inproceedings{bertsekas1995neuro,
  title={Neuro-dynamic programming: an overview},
  author={Bertsekas, Dimitri P and Tsitsiklis, John N},
  booktitle={Proceedings of 1995 34th IEEE Conference on Decision and Control},
  volume={1},
  pages={560--564},
  year={1995},
  organization={IEEE}
}

@inproceedings{wilson2007multi,
  title={Multi-task reinforcement learning: a hierarchical Bayesian approach},
  author={Wilson, Aaron and Fern, Alan and Ray, Soumya and Tadepalli, Prasad},
  booktitle={Proceedings of the 24th international conference on Machine learning},
  pages={1015--1022},
  year={2007}
}

@inproceedings{andreas2017modular,
  title={Modular multitask reinforcement learning with policy sketches},
  author={Andreas, Jacob and Klein, Dan and Levine, Sergey},
  booktitle={International Conference on Machine Learning},
  pages={166--175},
  year={2017},
  organization={PMLR}
}

@book{altman1999constrained,
  title={Constrained Markov decision processes},
  author={Altman, Eitan},
  volume={7},
  year={1999},
  publisher={CRC Press}
}

@article{mnih2013playing,
  title={Playing atari with deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1312.5602},
  year={2013}
}

@article{silver2016mastering,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={nature},
  volume={529},
  number={7587},
  pages={484},
  year={2016},
  publisher={Nature Publishing Group}
}

@inproceedings{hu1998multiagent,
  title={Multiagent reinforcement learning: theoretical framework and an algorithm.},
  author={Hu, Junling and Wellman, Michael P and others},
  booktitle={ICML},
  volume={98},
  pages={242--250},
  year={1998},
  organization={Citeseer}
}

% value based

@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}

@misc{watkins1989learning,
  title={Learning from Delayed Rewards. PhD thesis, University of Cambridge, Cambridge, England},
  author={Watkins, C},
  year={1989},
  publisher={May}
}

@book{boyd2004convex,
  title={Convex optimization},
  author={Boyd, Stephen and Boyd, Stephen P and Vandenberghe, Lieven},
  year={2004},
  publisher={Cambridge university press}
}

% policybased.tex

@INPROCEEDINGS{Ng99policyinvariance,
    author = {Andrew Y. Ng and Daishi Harada and Stuart Russell},
    title = {Policy invariance under reward transformations: Theory and application to reward shaping},
    booktitle = {In Proceedings of the Sixteenth International Conference on Machine Learning},
    year = {1999},
    pages = {278--287},
    publisher = {Morgan Kaufmann}
}
@article{schulman2015high,
  title={High-dimensional continuous control using generalized advantage estimation},
  author={Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1506.02438},
  year={2015}
}
@inproceedings{mnih2016asynchronous,
  title={Asynchronous methods for deep reinforcement learning},
  author={Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  booktitle={International conference on machine learning},
  pages={1928--1937},
  year={2016}
}
@inproceedings{grosse2016kronecker,
  title={A kronecker-factored approximate fisher matrix for convolution layers},
  author={Grosse, Roger and Martens, James},
  booktitle={International Conference on Machine Learning},
  pages={573--582},
  year={2016}
}

% policy gradient

@article{sutton1999policy,
  title={Policy gradient methods for reinforcement learning with function approximation},
  author={Sutton, Richard S and McAllester, David and Singh, Satinder and Mansour, Yishay},
  journal={Advances in neural information processing systems},
  volume={12},
  pages={1057--1063},
  year={1999}
}

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@article{greensmith2004variance,
  title={Variance reduction techniques for gradient estimates in reinforcement learning},
  author={Greensmith, Evan and Bartlett, Peter L and Baxter, Jonathan},
  journal={Journal of Machine Learning Research},
  volume={5},
  number={Nov},
  pages={1471--1530},
  year={2004}
}

@inproceedings{ng1999policy,
  title={Policy invariance under reward transformations: Theory and application to reward shaping},
  author={Ng, Andrew Y and Harada, Daishi and Russell, Stuart},
  booktitle={ICML},
  volume={99},
  pages={278--287},
  year={1999}
}

% trust region
@book{billingsley2008probability,
  title={Probability and measure},
  author={Billingsley, Patrick},
  year={2008},
  publisher={John Wiley \& Sons}
}

@inproceedings{schulman2015trust,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle={International conference on machine learning},
  pages={1889--1897},
  year={2015}
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@inproceedings{kakade2002approximately,
  title={Approximately optimal approximate reinforcement learning},
  author={Kakade, Sham and Langford, John},
  booktitle={ICML},
  volume={2},
  pages={267--274},
  year={2002}
}

@inproceedings{amari1998natural,
  title={Why natural gradient?},
  author={Amari, Shun-Ichi and Douglas, Scott C},
  booktitle={Proceedings of the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP'98 (Cat. No. 98CH36181)},
  volume={2},
  pages={1213--1216},
  year={1998},
  organization={IEEE}
}

@misc{shewchuk1994introduction,
  title={An introduction to the conjugate gradient method without the agonizing pain},
  author={Shewchuk, Jonathan Richard and others},
  year={1994},
  publisher={Carnegie-Mellon University. Department of Computer Science}
}

@article{vinyals2019alphastar,
  title={Alphastar: Mastering the real-time strategy game starcraft ii},
  author={Vinyals, Oriol and Babuschkin, Igor and Chung, Junyoung and Mathieu, Michael and Jaderberg, Max and Czarnecki, Wojciech M and Dudzik, Andrew and Huang, Aja and Georgiev, Petko and Powell, Richard and others},
  journal={DeepMind blog},
  pages={2},
  year={2019}
}

@article{akkaya2019solving,
  title={Solving rubik's cube with a robot hand},
  author={Akkaya, Ilge and Andrychowicz, Marcin and Chociej, Maciek and Litwin, Mateusz and McGrew, Bob and Petron, Arthur and Paino, Alex and Plappert, Matthias and Powell, Glenn and Ribas, Raphael and others},
  journal={arXiv preprint arXiv:1910.07113},
  year={2019}
}

@article{berner2019dota,
  title={Dota 2 with large scale deep reinforcement learning},
  author={Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and D{\k{e}}biak, Przemys{\l}aw and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and others},
  journal={arXiv preprint arXiv:1912.06680},
  year={2019}
}

% actor critic
@article{fujimoto2018addressing,
  title={Addressing function approximation error in actor-critic methods},
  author={Fujimoto, Scott and Van Hoof, Herke and Meger, David},
  journal={arXiv preprint arXiv:1802.09477},
  year={2018}
}
@article{haarnoja2018soft,
  title={Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1801.01290},
  year={2018}
}
@inproceedings{silver2014deterministic,
  title={Deterministic policy gradient algorithms},
  author={Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
  year={2014}
}

@inproceedings{konda2000actor,
  title={Actor-critic algorithms},
  author={Konda, Vijay R and Tsitsiklis, John N},
  booktitle={Advances in neural information processing systems},
  pages={1008--1014},
  year={2000}
}

@inproceedings{ho2016generative,
  title={Generative adversarial imitation learning},
  author={Ho, Jonathan and Ermon, Stefano},
  booktitle={Advances in neural information processing systems},
  pages={4565--4573},
  year={2016}
}

@inproceedings{andrychowicz2017hindsight,
  title={Hindsight experience replay},
  author={Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Abbeel, OpenAI Pieter and Zaremba, Wojciech},
  booktitle={Advances in neural information processing systems},
  pages={5048--5058},
  year={2017}
}

@article{feher2019hybrid,
  title={Hybrid DDPG Approach for Vehicle Motion Planning},
  author={Feh{\'e}r, {\'A}rp{\'a}d and Aradi, Szil{\'a}rd and Heged{\"u}s, Ferenc and B{\'e}csi, Tam{\'a}s and G{\'a}sp{\'a}r, P{\'e}ter},
  year={2019},
  publisher={SciTePress}
}

@article{haarnoja2017reinforcement,
  title={Reinforcement learning with deep energy-based policies},
  author={Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1702.08165},
  year={2017}
}

@inproceedings{todorov2008general,
  title={General duality between optimal control and estimation},
  author={Todorov, Emanuel},
  booktitle={2008 47th IEEE Conference on Decision and Control},
  pages={4286--4292},
  year={2008},
  organization={IEEE}
}
@article{kappen2005path,
  title={Path integrals and symmetry breaking for optimal control theory},
  author={Kappen, Hilbert J},
  journal={Journal of statistical mechanics: theory and experiment},
  volume={2005},
  number={11},
  pages={P11011},
  year={2005},
  publisher={IOP Publishing}
}
@article{schulman2017equivalence,
  title={Equivalence between policy gradients and soft q-learning},
  author={Schulman, John and Chen, Xi and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1704.06440},
  year={2017}
}

@misc{baselines,
  author = {Dhariwal, Prafulla and Hesse, Christopher and Klimov, Oleg and Nichol, Alex and Plappert, Matthias and Radford, Alec and Schulman, John and Sidor, Szymon and Wu, Yuhuai and Zhokhov, Peter},
  title = {OpenAI Baselines},
  year = {2017},
  publisher = {GitHub},
  journal = {GitHub repository},
}
@article{SpinningUp2018,
    author = {Achiam, Joshua},
    title = {{Spinning Up in Deep Reinforcement Learning}},
    year = {2018}
}

% gac
@article{dinh2016density,
  title={Density estimation using real nvp},
  author={Dinh, Laurent and Sohl-Dickstein, Jascha and Bengio, Samy},
  journal={arXiv preprint arXiv:1605.08803},
  year={2016}
}
@article{peyre2018computational,
  title={Computational optimal transport. arXiv e-prints, page},
  author={Peyr{\'e}, Gabriel and Cuturi, Marco},
  journal={arXiv preprint arXiv:1803.00567},
  year={2018}
}
@book{villani2008optimal,
  title={Optimal transport: old and new},
  author={Villani, C{\'e}dric},
  volume={338},
  year={2008},
  publisher={Springer Science \& Business Media}
}
@inproceedings{goodfellow2014generative,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  booktitle={Advances in neural information processing systems},
  pages={2672--2680},
  year={2014}
}
@inproceedings{gulrajani2017improved,
  title={Improved training of wasserstein gans},
  author={Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron C},
  booktitle={Advances in neural information processing systems},
  pages={5767--5777},
  year={2017}
}
@inproceedings{zhang2016generating,
  title={Generating text via adversarial training},
  author={Zhang, Yizhe and Gan, Zhe and Carin, Lawrence},
  booktitle={NIPS workshop on Adversarial Training},
  volume={21},
  year={2016}
}
@article{hoffman2013stochastic,
  title={Stochastic variational inference},
  author={Hoffman, Matthew D and Blei, David M and Wang, Chong and Paisley, John},
  journal={The Journal of Machine Learning Research},
  volume={14},
  number={1},
  pages={1303--1347},
  year={2013},
  publisher={JMLR. org}
}
@inproceedings{heess2015learning,
  title={Learning continuous control policies by stochastic value gradients},
  author={Heess, Nicolas and Wayne, Gregory and Silver, David and Lillicrap, Timothy and Erez, Tom and Tassa, Yuval},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2944--2952},
  year={2015}
}
@misc{openaigym,
  Author = {Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
  Title = {OpenAI Gym},
  Year = {2016},
  Eprint = {arXiv:1606.01540},
}
@inproceedings{todorov2012mujoco,
  title={Mujoco: A physics engine for model-based control},
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  pages={5026--5033},
  year={2012},
  organization={IEEE}
}
@article{engstrom2020implementation,
  title={Implementation Matters in Deep Policy Gradients: A Case Study on PPO and TRPO},
  author={Engstrom, Logan and Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Janoos, Firdaus and Rudolph, Larry and Madry, Aleksander},
  journal={arXiv preprint arXiv:2005.12729},
  year={2020}
}
@inproceedings{duan2016benchmarking,
  title={Benchmarking deep reinforcement learning for continuous control},
  author={Duan, Yan and Chen, Xi and Houthooft, Rein and Schulman, John and Abbeel, Pieter},
  booktitle={International Conference on Machine Learning},
  pages={1329--1338},
  year={2016}
}

% mmd
@inproceedings{arbel2019maximum,
  title={Maximum mean discrepancy gradient flow},
  author={Arbel, Michael and Korba, Anna and Salim, Adil and Gretton, Arthur},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6484--6494},
  year={2019}
}
@article{gretton2012kernel,
  title={A kernel two-sample test},
  author={Gretton, Arthur and Borgwardt, Karsten M and Rasch, Malte J and Sch{\"o}lkopf, Bernhard and Smola, Alexander},
  journal={The Journal of Machine Learning Research},
  volume={13},
  number={1},
  pages={723--773},
  year={2012},
  publisher={JMLR. org}
}

% safe rl
@phdthesis{thomas2015safe,
  title={Safe reinforcement learning},
  author={Thomas, Philip S},
  year={2015},
  school={University of Massachusetts Libraries}
}
@article{Ray2019,
    author = {Ray, Alex and Achiam, Joshua and Amodei, Dario},
    title = {{Benchmarking Safe Exploration in Deep Reinforcement Learning}},
    year = {2019}
}
@inproceedings{berkenkamp2017safe,
  title={Safe model-based reinforcement learning with stability guarantees},
  author={Berkenkamp, Felix and Turchetta, Matteo and Schoellig, Angela and Krause, Andreas},
  booktitle={Advances in neural information processing systems},
  pages={908--918},
  year={2017}
}
@inproceedings{christiano2017deep,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4299--4307},
  year={2017}
}
@article{christiano2018supervising,
  title={Supervising strong learners by amplifying weak experts},
  author={Christiano, Paul and Shlegeris, Buck and Amodei, Dario},
  journal={arXiv preprint arXiv:1810.08575},
  year={2018}
}
@inproceedings{fisac2019bridging,
  title={Bridging hamilton-jacobi safety analysis and reinforcement learning},
  author={Fisac, Jaime F and Lugovoy, Neil F and Rubies-Royo, Vicen{\c{c}} and Ghosh, Shromona and Tomlin, Claire J},
  booktitle={2019 International Conference on Robotics and Automation (ICRA)},
  pages={8550--8556},
  year={2019},
  organization={IEEE}
}
@inproceedings{gehring2013smart,
  title={Smart exploration in reinforcement learning using absolute temporal difference errors},
  author={Gehring, Clement and Precup, Doina},
  booktitle={Proceedings of the 2013 international conference on Autonomous agents and multi-agent systems},
  pages={1037--1044},
  year={2013}
}
@article{gauci2018horizon,
  title={Horizon: Facebook's open source applied reinforcement learning platform},
  author={Gauci, Jason and Conti, Edoardo and Liang, Yitao and Virochsiri, Kittipat and He, Yuchen and Kaden, Zachary and Narayanan, Vivek and Ye, Xiaohui and Chen, Zhengxing and Fujimoto, Scott},
  journal={arXiv preprint arXiv:1811.00260},
  year={2018}
}
@inproceedings{hans2008safe,
  title={Safe exploration for reinforcement learning.},
  author={Hans, Alexander and Schneega{\ss}, Daniel and Sch{\"a}fer, Anton Maximilian and Udluft, Steffen},
  booktitle={ESANN},
  pages={143--148},
  year={2008}
}
@inproceedings{hadfield2016cooperative,
  title={Cooperative inverse reinforcement learning},
  author={Hadfield-Menell, Dylan and Russell, Stuart J and Abbeel, Pieter and Dragan, Anca},
  booktitle={Advances in neural information processing systems},
  pages={3909--3917},
  year={2016}
}
@article{irving2018ai,
  title={AI safety via debate},
  author={Irving, Geoffrey and Christiano, Paul and Amodei, Dario},
  journal={arXiv preprint arXiv:1805.00899},
  year={2018}
}
@article{krakovna2018penalizing,
  title={Penalizing side effects using stepwise relative reachability},
  author={Krakovna, Victoria and Orseau, Laurent and Kumar, Ramana and Martic, Miljan and Legg, Shane},
  journal={arXiv preprint arXiv:1806.01186},
  year={2018}
}
@article{leike2017ai,
  title={ai safety gridworlds},
  author={leike, jan and martic, miljan and krakovna, victoria and ortega, pedro a and everitt, tom and lefrancq, andrew and orseau, laurent and legg, shane},
  journal={arxiv preprint arxiv:1711.09883},
  year={2017}
}
@article{moldovan2012safe,
  title={Safe exploration in markov decision processes},
  author={Moldovan, Teodor Mihai and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1205.4810},
  year={2012}
}
@inproceedings{pecka2014safe,
  title={Safe exploration techniques for reinforcement learning--an overview},
  author={Pecka, Martin and Svoboda, Tomas},
  booktitle={International Workshop on Modelling and Simulation for Autonomous Systems},
  pages={357--375},
  year={2014},
  organization={Springer}
}
@inproceedings{pirotta2013safe,
  title={Safe policy iteration},
  author={Pirotta, Matteo and Restelli, Marcello and Pecorino, Alessio and Calandriello, Daniele},
  booktitle={International Conference on Machine Learning},
  pages={307--315},
  year={2013}
}
@article{papini2019smoothing,
  title={Smoothing policies and safe policy gradients},
  author={Papini, Matteo and Pirotta, Matteo and Restelli, Marcello},
  journal={arXiv preprint arXiv:1905.03231},
  year={2019}
}
@article{achiam2017constrained,
  title={Constrained policy optimization},
  author={Achiam, Joshua and Held, David and Tamar, Aviv and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1705.10528},
  year={2017}
}